# Khora Adapter

**Khora Adapter** provides a unified interface for interacting with multiple large language model (LLM) APIs. It abstracts away provider-specific quirks, allowing your orchestration tools to speak a consistent prompt-and-response protocolâ€”even as underlying LLMs evolve.

---

## âœ¨ Features

- âœ… Provider abstraction (OpenAI, Gemini, etc.)
- â˜ Role-aware, multi-message prompts 
- â˜ Consistent metadata in responses
- âœ… Lazy import and dynamic API key resolution
- â˜ Streaming support
- âœ… Fully typed and test-covered
- âœ… Extensible with new models and protocols

---

## ğŸ“¦ Installation

```bash
poetry install
cp .env.example .env  # Then add your API keys
````

---

## ğŸš€ Usage

```bash
poetry run khora_adapter --provider openai --model gpt-4 --text "What's the capital of France?"
```

If `--text` is omitted, it will prompt you interactively.

### Example: Multi-role Chat Prompt

```bash
poetry run khora_adapter --provider openai --model gpt-4
```

Then enter a structured prompt when prompted:

```plaintext
>>> [system] You are a helpful assistant.
>>> [user] What's the capital of France?
>>> [assistant] The capital of France is Paris.
>>> [user] And what's the capital of Germany?
```

Multi-turn prompts and roles are parsed internally or passed as structured messages from an upstream client.

---

## ğŸ§± Python API

```python
from khora_adapter.prompt import run
from khora_adapter.llms.registry import get_factory
from khora_adapter.messages import PromptMessage

factory = get_factory(args)
model = factory.build()

response = run(
    messages=[
        PromptMessage(role="system", content="You are a helpful assistant."),
        PromptMessage(role="user", content="Explain photosynthesis."),
    ],
    model=model,
)

print(response.text)
print(response.usage)  # Optional metadata like token count
```

---

## ğŸ’¬ Structured Prompt Format

Prompts are internally converted to a list of messages like:

```python
[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What's 2 + 2?"},
]
```

These are supported natively by OpenAI, Gemini, and others.

---

## ğŸŒŠ Streaming Support

For models and clients that support it:

```python
for chunk in model.stream(messages):
    print(chunk, end="", flush=True)
```

Streaming lets downstream UIs or tools render partial output in real time.

---

## ğŸ“Š Structured Output

Responses return structured metadata:

```python
@dataclass
class LLMResponse:
    text: str
    usage: dict[str, Any] | None = None  # tokens, cost, etc.
    raw: Any = None  # Original provider response
```

> For advanced formatting (Markdown, JSON, HTML), use dedicated rendering tools or downstream formatters. Khora only provides clean, structured output.

---

## ğŸ›  Extending with New Providers

To add a new model:

1. Create `khora_adapter/llms/<provider>/model.py` and `factory.py`
2. Implement the `LLM` protocol
3. Register the factory in `khora_adapter.llms.registry`

Khora handles the rest.

---

## ğŸ§ª Testing

```bash
poetry run pytest --cov
```

---

## ğŸ§  Philosophy

Khora is designed to stay small and stable. It **does not** attempt prompt engineering, retries, formatting, or tool use â€” these are the job of orchestration layers built *on top* of Khora.

Instead, Khora ensures:

* One interface to talk to any LLM
* Strong typing and safety
* Easy extension
* Predictable behavior

---

## ğŸ“ Project Structure

```
khora_adapter/
â”œâ”€â”€ cli.py                  # CLI entrypoint
â”œâ”€â”€ prompt.py               # Main invocation logic
â”œâ”€â”€ protocols/              # Interfaces (LLM, LLMFactory)
â”œâ”€â”€ llms/                   # Provider implementations
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ registry.py
â”‚   â”œâ”€â”€ gemini/
â”‚   â””â”€â”€ openai/
tests/
scripts/
```

---

## ğŸ— Environment Variables

Place your API keys in `.env`:

```
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=...
```

---

## ğŸ“¬ Coming Soon

* â˜ Prompt templating (Jinja-like)
* â˜ Token counting utility
* â˜ Retry/backoff middleware
* â˜ Tool use + function calling support
* â˜ More providers (Claude, Mistral, Groq, Llama.cpp)

---

## License

This project is licensed under the [GNU Affero General Public License v3.0](https://www.gnu.org/licenses/agpl-3.0.html).

AGPLv3 is a strong copyleft license designed to ensure end users can access the full source code of server-side software.

### Commercial Licensing

If you are a for-profit company interested in a commercial license that permits use without AGPL obligations, please [contact the author](mailto:sudoku-king-gender@duck.com).

(Note: Dual licensing is not currently offered, but we reserve the right to do so in the future.)
